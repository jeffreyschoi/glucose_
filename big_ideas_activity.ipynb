{"cells":[{"cell_type":"code","execution_count":null,"id":"e1a1caa9-acef-4302-bc62-2b52220e6a07","metadata":{"id":"e1a1caa9-acef-4302-bc62-2b52220e6a07"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import matplotlib.pyplot as plt\n","from sklearn import metrics\n","from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score\n","from sklearn.model_selection import train_test_split, LeaveOneOut\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier"]},{"cell_type":"code","execution_count":null,"id":"f721b767-7f3a-42e0-90ab-b37e6cdd5f16","metadata":{"id":"f721b767-7f3a-42e0-90ab-b37e6cdd5f16"},"outputs":[],"source":["\"\"\"This file runs our ML model fit from the PPG_DaLia dataset onto the Big Ideas dataset.\n","        - we train a random forest model with 300 trees\n","        - we also show the average accuracy, precision, and recall score of this classifier using Leave-One-Individuals-Out Cross Validation\n","        - we condense no_ activity to contain baseline, clean_baseline, lunch, working, driving\n","        - we condense activity to contain stairs, soccer, cycling, walking\n","        - after applying our model on the Big Ideas dataset, we identify periods if they are more than 5 minutes\n","        - combined identified periods if there were less than a minute in difference and deleted if food was consumed an hour before or after\"\"\""]},{"cell_type":"code","execution_count":null,"id":"bbba1612-238e-4f12-aebf-dabc5b945989","metadata":{"id":"bbba1612-238e-4f12-aebf-dabc5b945989","outputId":"6d413a5c-dc37-4ec6-e42e-f059d0599bb2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Subject 1 Combined Data Length: 1324\n","Subject 2 Combined Data Length: 1181\n","Subject 3 Combined Data Length: 1254\n","Subject 4 Combined Data Length: 1302\n","Subject 5 Combined Data Length: 1328\n","Subject 6 Combined Data Length: 1413\n","Subject 7 Combined Data Length: 1334\n","Subject 8 Combined Data Length: 1160\n","Subject 9 Combined Data Length: 1232\n","Subject 10 Combined Data Length: 1532\n","Subject 11 Combined Data Length: 1299\n","Subject 12 Combined Data Length: 1126\n","Subject 13 Combined Data Length: 1312\n","Subject 14 Combined Data Length: 1280\n","Subject 15 Combined Data Length: 1139\n","Subject 1 Combined Data Length: 73779\n","Subject 2 Combined Data Length: 84518\n","Subject 3 Combined Data Length: 2232\n","Subject 4 Combined Data Length: 35926\n","Subject 5 Combined Data Length: 96772\n","Subject 6 Combined Data Length: 59134\n","Subject 7 Combined Data Length: 87953\n","Subject 8 Combined Data Length: 83423\n","Subject 9 Combined Data Length: 82157\n","Subject 10 Combined Data Length: 89398\n","Subject 11 Combined Data Length: 69695\n","Subject 12 Combined Data Length: 78368\n","Subject 13 Combined Data Length: 95268\n","Subject 14 Combined Data Length: 15957\n","Subject 15 Combined Data Length: 14375\n","Subject 16 Combined Data Length: 66403\n"]}],"source":["\"\"\" Combines biosignal csvs into one dataframe and saves in a dictionary \"\"\"\n","ppg_dataframes = {}\n","for i in np.arange(1, 16):\n","    name = f\"data{i}\"\n","    acc = pd.read_csv(f\"D:/REU_2024/PPG/S{i}/S{i}_E4/ACC_specific_windowed_features.csv\") # simple windowed biosignals with activity from ppg dataset\n","    hr = pd.read_csv(f\"D:/REU_2024/PPG/S{i}/S{i}_E4/HR_specific_windowed_features.csv\")\n","    bvp = pd.read_csv(f\"D:/REU_2024/PPG/S{i}/S{i}_E4/BVP_specific_windowed_features.csv\")\n","    acc['start_time'] = pd.to_datetime(acc['start_time'])\n","    hr['start_time'] = pd.to_datetime(hr['start_time'])\n","    bvp['start_time'] = pd.to_datetime(bvp['start_time'])\n","    acc['end_time'] = pd.to_datetime(acc['end_time'])\n","    hr['end_time'] = pd.to_datetime(hr['end_time'])\n","    bvp['end_time'] = pd.to_datetime(bvp['end_time'])\n","\n","    common_columns = ['start_time', 'end_time', 'activity']\n","    data = acc.merge(hr, on=common_columns, how='inner').merge(bvp, on=common_columns, how='inner')\n","    print(f\"Subject {i} Combined Data Length: {len(data)}\")\n","\n","    data = data[~data['activity'].isin([0])]\n","    data.loc[data['activity'].isin([1, 2, 7, 8, 9]), 'activity'] = 0\n","    data.loc[data['activity'].isin([3, 4, 5, 6]), 'activity'] = 1\n","    ppg_dataframes[name] = data\n","\n","bigideas_dataframes = {}\n","for i in np.arange(1, 17):\n","    if i < 10:\n","        name = f\"data{i}\"\n","        acc = pd.read_csv(f\"D:/REU_2024/BigIdeas/00{i}/ACC_00{i}_simple_windowed.csv\") # simple windowed biosignals from big ideas dataset\n","        hr = pd.read_csv(f\"D:/REU_2024/BigIdeas/00{i}/HR_00{i}_simple_windowed.csv\")\n","        bvp = pd.read_csv(f\"D:/REU_2024/BigIdeas/00{i}/BVP_00{i}_simple_windowed.csv\")\n","    if i > 9:\n","        name = f\"data{i}\"\n","        acc = pd.read_csv(f\"D:/REU_2024/BigIdeas/0{i}/ACC_0{i}_simple_windowed.csv\")\n","        hr = pd.read_csv(f\"D:/REU_2024/BigIdeas/0{i}/HR_0{i}_simple_windowed.csv\")\n","        bvp = pd.read_csv(f\"D:/REU_2024/BigIdeas/0{i}/BVP_0{i}_simple_windowed.csv\")\n","    acc['start_time'] = pd.to_datetime(acc['start_time'])\n","    hr['start_time'] = pd.to_datetime(hr['start_time'])\n","    bvp['start_time'] = pd.to_datetime(bvp['start_time'])\n","    acc['end_time'] = pd.to_datetime(acc['end_time'])\n","    hr['end_time'] = pd.to_datetime(hr['end_time'])\n","    bvp['end_time'] = pd.to_datetime(bvp['end_time'])\n","\n","    common_columns = ['start_time', 'end_time']\n","    data = acc.merge(hr, on=common_columns, how='inner').merge(bvp, on=common_columns, how='inner')\n","    data = data.dropna() # removes rows with missing data due to differing collection frequencies\n","    print(f\"Subject {i} Combined Data Length: {len(data)}\")\n","\n","    data.columns = data.columns.str.replace(' ', '', regex=False)\n","    bigideas_dataframes[name] = data"]},{"cell_type":"code","execution_count":null,"id":"0fe2b895-897d-43dd-a924-902671f95849","metadata":{"id":"0fe2b895-897d-43dd-a924-902671f95849","outputId":"8f6f17d6-756c-4df2-961b-ed1239af80cd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Testing on data1\n","data1: Accuracy = 0.9313, Precision = 0.8811, Recall = 0.8431\n","Testing on data2\n","data2: Accuracy = 0.9079, Precision = 0.8086, Recall = 0.8846\n","Testing on data3\n","data3: Accuracy = 0.9138, Precision = 0.8179, Recall = 0.8740\n","Testing on data4\n","data4: Accuracy = 0.9333, Precision = 0.8906, Recall = 0.8708\n","Testing on data5\n","data5: Accuracy = 0.8267, Precision = 0.6566, Recall = 0.6905\n","Testing on data6\n","data6: Accuracy = 0.9442, Precision = 0.9208, Recall = 0.8873\n","Testing on data7\n","data7: Accuracy = 0.9371, Precision = 0.8893, Recall = 0.8662\n","Testing on data8\n","data8: Accuracy = 0.8278, Precision = 0.7204, Recall = 0.5776\n","Testing on data9\n","data9: Accuracy = 0.8716, Precision = 0.7953, Recall = 0.7426\n","Testing on data10\n","data10: Accuracy = 0.8228, Precision = 0.8261, Recall = 0.5723\n","Testing on data11\n","data11: Accuracy = 0.9203, Precision = 0.8642, Recall = 0.8358\n","Testing on data12\n","data12: Accuracy = 0.9384, Precision = 0.8783, Recall = 0.9167\n","Testing on data13\n","data13: Accuracy = 0.9132, Precision = 0.8672, Recall = 0.8275\n","Testing on data14\n","data14: Accuracy = 0.9051, Precision = 0.8987, Recall = 0.7584\n","Testing on data15\n","data15: Accuracy = 0.9178, Precision = 0.8893, Recall = 0.8346\n","Overall Average Accuracy: 0.9007\n","Overall Average Precision: 0.8403\n","Overall Average Recall: 0.7988\n"]}],"source":["\"\"\" Shows random forest accuracy, precision, recall in supervised setting using LOOCV\"\"\"\n","accuracies = []\n","precisions = []\n","recalls = []\n","\n","dataset_names = list(ppg_dataframes.keys())\n","\n","for i in range(len(dataset_names)):\n","    test_name = dataset_names[i]\n","    train_names = dataset_names[:i] + dataset_names[i+1:]\n","\n","    print(f\"Testing on {test_name}\")\n","\n","    train_data = pd.concat([ppg_dataframes[name] for name in train_names], ignore_index=True)\n","    test_data = ppg_dataframes[test_name]\n","\n","    X_train, y_train = train_data.drop(columns=['start_time', 'end_time', 'activity']), train_data['activity']\n","    X_test, y_test = test_data.drop(columns=['start_time', 'end_time', 'activity']), test_data['activity']\n","\n","    model = RandomForestClassifier(n_estimators=300, random_state=42)\n","    model.fit(X_train, y_train)\n","\n","    y_pred = model.predict(X_test)\n","\n","    acc = accuracy_score(y_test, y_pred) # correct classified over total data\n","    prec = precision_score(y_test, y_pred, average='binary', zero_division=1) # how correct our positive predictions are\n","    recall = recall_score(y_test, y_pred, average='binary', zero_division=1) # how well our model identified positives\n","\n","    accuracies.append(acc)\n","    precisions.append(prec)\n","    recalls.append(recall)\n","\n","    print(f\"{test_name}: Accuracy = {acc:.4f}, Precision = {prec:.4f}, Recall = {recall:.4f}\")\n","\n","average_accuracy = np.mean(accuracies)\n","average_precision = np.mean(precisions)\n","average_recall = np.mean(recalls)\n","\n","print(f\"Overall Average Accuracy: {average_accuracy:.4f}\")\n","print(f\"Overall Average Precision: {average_precision:.4f}\")\n","print(f\"Overall Average Recall: {average_recall:.4f}\")"]},{"cell_type":"code","execution_count":null,"id":"4bb1d828-3197-4549-a72f-7164bc986331","metadata":{"id":"4bb1d828-3197-4549-a72f-7164bc986331","outputId":"9176f185-4cdc-461d-9533-81004595abe7"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\jcehf\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Dataset data1 predicted\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\jcehf\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Dataset data2 predicted\n","Dataset data3 predicted\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\jcehf\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n","  warnings.warn(\n","C:\\Users\\jcehf\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Dataset data4 predicted\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\jcehf\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Dataset data5 predicted\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\jcehf\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Dataset data6 predicted\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\jcehf\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Dataset data7 predicted\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\jcehf\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Dataset data8 predicted\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\jcehf\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Dataset data9 predicted\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\jcehf\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Dataset data10 predicted\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\jcehf\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Dataset data11 predicted\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\jcehf\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Dataset data12 predicted\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\jcehf\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Dataset data13 predicted\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\jcehf\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Dataset data14 predicted\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\jcehf\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Dataset data15 predicted\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\jcehf\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Dataset data16 predicted\n"]}],"source":["\"\"\" Trains random forest on whole PPG-DaLia dataset and applies on Big Ideas dataset \"\"\"\n","ppg_features = pd.concat([df.drop(columns=['start_time','end_time','activity']) for df in ppg_dataframes.values()], axis=0)\n","ppg_activity = pd.concat([df['activity'] for df in ppg_dataframes.values()], axis=0)\n","sc = StandardScaler()\n","ppg_features = sc.fit_transform(ppg_features)\n","\n","rf = RandomForestClassifier(n_estimators=300)\n","rf.fit(ppg_features, ppg_activity)\n","\n","for key, df in bigideas_dataframes.items():\n","    bigideas_features = df.drop(columns=['start_time', \"end_time\"]).to_numpy()\n","    bigideas_features = sc.transform(bigideas_features) # normalizes big ideas dataset according to ppg dataset metrics\n","\n","    bigideas_activity = rf.predict(bigideas_features)\n","\n","    bigideas_dataframes[key]['activity'] = bigideas_activity\n","    print(f\"Dataset {key} predicted\")"]},{"cell_type":"code","execution_count":null,"id":"93457ac2-5a45-4942-9140-13caf30bfb5b","metadata":{"id":"93457ac2-5a45-4942-9140-13caf30bfb5b","outputId":"869b79f0-5b73-4fe8-e232-2560ec54f598"},"outputs":[{"name":"stdout","output_type":"stream","text":["Person 1 saved!\n","Person 2 saved!\n","Person 3 saved!\n","Person 4 saved!\n","Person 5 saved!\n","Person 6 saved!\n","Person 7 saved!\n","Person 8 saved!\n","Person 9 saved!\n","Person 10 saved!\n","Person 11 saved!\n","Person 12 saved!\n","Person 13 saved!\n","Person 14 saved!\n","Person 15 saved!\n","Person 16 saved!\n"]}],"source":["\"\"\" Saves Big Ideas dataset including activity \"\"\"\n","i = 1\n","for key, df in bigideas_dataframes.items():\n","    reduced_df = df[['start_time', 'end_time', 'activity']]\n","    if i < 10:\n","        file = f\"D:/REU_2024/BigIdeas/00{i}/windowed_activity.csv\" # edit file path accordingly\n","    if i > 9:\n","        file = f\"D:/REU_2024/BigIdeas/0{i}/windowed_activity.csv\"\n","\n","    if os.path.exists(file):\n","        os.remove(file)\n","    reduced_df.to_csv(file, index=False)\n","    print(f\"Person {i} saved!\")\n","    i += 1"]},{"cell_type":"code","execution_count":null,"id":"c5a31ecd-8c59-4fbc-a821-69696aa814a2","metadata":{"id":"c5a31ecd-8c59-4fbc-a821-69696aa814a2"},"outputs":[],"source":["\"\"\" Defines funtion to grab long activity periods, limiting to 5 min, differences in interval time as 1 min, and removes food as confounder\"\"\"\n","def filter_activity_intervals(file_path, i, min_duration=300, diff_threshold=60):\n","        # Load CSV into DataFrame\n","    df = pd.read_csv(file_path)\n","\n","    df['start_time'] = pd.to_datetime(df['start_time'])\n","    df['end_time'] = pd.to_datetime(df['end_time'])\n","    if i < 10:\n","        food = pd.read_csv(f\"D:/REU_2024/BigIdeas/00{i}/Food_Log_00{i}.csv\")\n","    if i > 9:\n","        food = pd.read_csv(f\"D:/REU_2024/BigIdeas/0{i}/Food_Log_0{i}.csv\")\n","\n","    intervals = []\n","    start = None\n","\n","    for i, row in df.iterrows():\n","        if row['activity'] == 1:\n","            if start is None:\n","                start = row['start_time']\n","            end = row['end_time']\n","        else:\n","            if start is not None:\n","                intervals.append((start, end))\n","                start = None\n","\n","    if start is not None:\n","        intervals.append((start, end))\n","\n","    merged_intervals = []\n","    prev_start, prev_end = intervals[0]\n","\n","    for start, end in intervals[1:]:\n","        if (start - prev_end).total_seconds() < diff_threshold:\n","            prev_end = end\n","        else:\n","            merged_intervals.append((prev_start, prev_end))\n","            prev_start, prev_end = start, end\n","\n","    merged_intervals.append((prev_start, prev_end))\n","\n","    final_intervals = [(s, e) for s, e in merged_intervals if (e - s).total_seconds() >= min_duration]\n","\n","    food['datetime'] = pd.to_datetime(food['date'] + ' ' + food['time'])\n","    food_times = food['datetime']\n","    filtered_intervals = []\n","\n","    for start, end in final_intervals:\n","        if not any((start - pd.Timedelta(hours=1) <= t <= start) or (end <= t <= end + pd.Timedelta(hours=1)) for t in food_times):\n","            filtered_intervals.append((start, end))\n","\n","    # Create DataFrame\n","    result_df = pd.DataFrame(filtered_intervals, columns=['start_time', 'end_time'])\n","\n","    return result_df\n"]},{"cell_type":"code","execution_count":null,"id":"42fc261c-9fa9-406c-8843-c854b0c0c9a6","metadata":{"id":"42fc261c-9fa9-406c-8843-c854b0c0c9a6","outputId":"92352625-2d49-4c5b-f254-faafd193dd91"},"outputs":[{"name":"stdout","output_type":"stream","text":["(6, 2)\n","(29, 2)\n","(0, 2)\n","(20, 2)\n","(7, 2)\n","(10, 2)\n","(16, 2)\n","(65, 2)\n","(33, 2)\n","(8, 2)\n","(35, 2)\n","(2, 2)\n","(30, 2)\n","(0, 2)\n","(43, 2)\n","(53, 2)\n"]}],"source":["for i in range(1,17):\n","    if i < 10:\n","        data = f\"D:/REU_2024/BigIdeas/00{i}/windowed_activity.csv\"\n","    if i > 9:\n","        data = f\"D:/REU_2024/BigIdeas/0{i}/windowed_activity.csv\"\n","\n","    print(filter_activity_intervals(data, i).shape)"]},{"cell_type":"code","execution_count":null,"id":"80b5307a-7a3f-49cd-82c7-5b2bf5258623","metadata":{"id":"80b5307a-7a3f-49cd-82c7-5b2bf5258623","outputId":"9fb20097-906e-4d4e-9c29-47738cdddccf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Person 1 activity saved!\n","Person 2 activity saved!\n","Person 3 activity saved!\n","Person 4 activity saved!\n","Person 5 activity saved!\n","Person 6 activity saved!\n","Person 7 activity saved!\n","Person 8 activity saved!\n","Person 9 activity saved!\n","Person 10 activity saved!\n","Person 11 activity saved!\n","Person 12 activity saved!\n","Person 13 activity saved!\n","Person 14 activity saved!\n","Person 15 activity saved!\n","Person 16 activity saved!\n"]}],"source":["for i in range(1,17):\n","    if i < 10:\n","        data = f\"D:/REU_2024/BigIdeas/00{i}/windowed_activity.csv\" # edit where to grab data\n","        file = f\"D:/REU_2024/BigIdeas/00{i}/activity_bouts.csv\" # and where to save data\n","    if i > 9:\n","        data = f\"D:/REU_2024/BigIdeas/0{i}/windowed_activity.csv\"\n","        file = f\"D:/REU_2024/BigIdeas/0{i}/activity_bouts.csv\"\n","\n","    if os.path.exists(file):\n","        os.remove(file)\n","    filter_activity_intervals(data, i).to_csv(file, index=False)\n","    print(f\"Person {i} activity saved!\")\n","    i += 1"]},{"cell_type":"code","execution_count":null,"id":"893ded05-374b-4e5c-bc6e-6c6dff11fa6f","metadata":{"id":"893ded05-374b-4e5c-bc6e-6c6dff11fa6f"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}