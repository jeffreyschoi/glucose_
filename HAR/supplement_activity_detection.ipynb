{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"m-nqtAulH521","executionInfo":{"status":"ok","timestamp":1720656810101,"user_tz":240,"elapsed":613,"user":{"displayName":"Duy Thuc Nguyen","userId":"06305071281301379745"}},"outputId":"dc9714e3-42b4-4042-8bfd-ecabd9447fb2","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stderr","text":["/home/d_nguyen11/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n","  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"markdown","metadata":{"id":"pqMUmE8TIjE8"},"source":["To mark out activity bouts, we follow the following steps:\n","- Fourier Transform the data into frequency domain\n","- Filter noises\n","- Apply the trained model in improved_classification_model.ipynb\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qANCJCDoIgm9"},"outputs":[],"source":["# We first want to find continuous intervals with data\n","def interval_grabber(df):\n","  df.columns = ['datetime', 'acc_x', 'acc_y', 'acc_z']\n","\n","  interval_dfs = []\n","  # For acc_x\n","  df['interval'] = (df['acc_x'].notna() != df['acc_x'].notna().shift()).cumsum()\n","  # Create new dataframes for each interval\n","  x_interval_dfs = {}\n","  for interval_id, interval_group in df.groupby('interval'):\n","    if interval_group['acc_z'].notna().any():\n","      x_interval_dfs[interval_id] = interval_group.drop(columns='interval')\n","  interval_dfs.append(x_interval_dfs)\n","\n","  # For acc_y\n","  df['interval'] = (df['acc_y'].notna() != df['acc_y'].notna().shift()).cumsum()\n","  # Create a new dataframe for each interval\n","  y_interval_dfs = {}\n","  for interval_id, interval_group in df.groupby('interval'):\n","    if interval_group['acc_y'].notna().any():\n","      y_interval_dfs[interval_id] = interval_group.drop(columns='interval')\n","  interval_dfs.append(y_interval_dfs)\n","\n","  # For acc_z\n","  df['interval'] = (df['acc_z'].notna() != df['acc_z'].notna().shift()).cumsum()\n","  # Create a new dataframe for each interval\n","  z_interval_dfs = {}\n","  for interval_id, interval_group in df.groupby('interval'):\n","    if interval_group['acc_z'].notna().any():\n","      z_interval_dfs[interval_id] = interval_group.drop(columns='interval')\n","  interval_dfs.append(z_interval_dfs)\n","\n","  return interval_dfs"]},{"cell_type":"markdown","metadata":{"id":"6xxXHk_zIyiM"},"source":["We now apply Fourier transform and filter out noise"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xva2E_2WJru1"},"outputs":[],"source":["def denoise(df):\n","  # For each accelerometry component, we take the 17 highest signals.\n","\n","  n = len(df['acc_x'])\n","  f_hat = np.fft.fft(df['acc_x'])\n","  PSD = f_hat * np.conj(f_hat) / n\n","  freq = (1/n) * np.arange(n)\n","  min_amplitude = np.partition(PSD, -17)[-17]\n","  indices = PSD >= min_amplitude\n","  PSD_filtered = PSD * indices\n","  f_hat_filtered = f_hat * indices\n","  f_filtered = np.fft.ifft(f_hat_filtered)\n","  df['denoised_acc_x'] = f_filtered\n","\n","  n = len(df['acc_y'])\n","  f_hat = np.fft.fft(df['acc_y'])\n","  PSD = f_hat * np.conj(f_hat) / n\n","  freq = (1/n) * np.arange(n)\n","  min_amplitude = np.partition(PSD, -17)[-17]\n","  indices = PSD >= min_amplitude\n","  PSD_filtered = PSD * indices\n","  f_hat_filtered = f_hat * indices\n","  f_filtered = np.fft.ifft(f_hat_filtered)\n","  df['denoised_acc_y'] = f_filtered\n","\n","  n = len(df['acc_z'])\n","  f_hat = np.fft.fft(df['acc_z'])\n","  PSD = f_hat * np.conj(f_hat) / n\n","  freq = (1/n) * np.arange(n)\n","  min_amplitude = np.partition(PSD, -17)[-17]\n","  indices = PSD >= min_amplitude\n","  PSD_filtered = PSD * indices\n","  f_hat_filtered = f_hat * indices\n","  f_filtered = np.fft.ifft(f_hat_filtered)\n","  df['denoised_acc_z'] = f_filtered\n","\n","  df['denoised_acc_x'] = df['denoised_acc_x'].apply(lambda x: x.real)\n","  df['denoised_acc_y'] = df['denoised_acc_y'].apply(lambda x: x.real)\n","  df['denoised_acc_z'] = df['denoised_acc_z'].apply(lambda x: x.real)\n","\n","  df.drop(columns=['acc_x', 'acc_y', 'acc_z'], inplace=True)\n","  return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n8jVbf4-vDpz"},"outputs":[],"source":["# df = pd.read_csv('/content/drive/MyDrive/ResearchProject/datafiles/001/ACC_001.csv')\n","# df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6cmcC4dIvp6N"},"outputs":[],"source":["#df.columns = ['datetime', 'acc_x', 'acc_y', 'acc_z']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-kEqDrjgyKir"},"outputs":[],"source":["import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"92OPlZivKEV0"},"outputs":[],"source":["def denoising_intervals_old(ind):\n","  folder_path = f'/content/drive/MyDrive/ResearchProject/datafiles/{ind}/segmented'\n","  all_files = os.listdir(folder_path)\n","  csv_files = [file for file in all_files if file.endswith('.csv')]\n","\n","  new_folder_name = \"denoised_interval\"\n","  new_folder_path = os.path.join(f'/content/drive/MyDrive/ResearchProject/datafiles/{ind}', new_folder_name)\n","  os.makedirs(new_folder_path, exist_ok=True)\n","\n","  for file in csv_files:\n","    df = pd.read_csv(os.path.join(folder_path, file))\n","    day = df['day'].iloc[0]\n","    component_list = interval_grabber(df)\n","    for interval_dfs in component_list:\n","      for interval_id, interval_df in interval_dfs.items():\n","        interval_df = denoise(interval_df)\n","        #interval_df = interval_df.drop(columns=['acc_x', 'acc_y', 'acc_z'])\n","        interval_df.to_csv(f'/content/drive/MyDrive/ResearchProject/datafiles/{ind}/denoised_interval/denoised_ACC_{ind}_day{day}_{interval_id}.csv')\n","\n","def denoising_intervals(ind):\n","  folder_path = f\"/home/d_nguyen11/Documents/reu_stats_2024/datafiles/{ind}\"\n","  df = pd.read_csv(os.path.join(folder_path, f'ACC_{ind}.csv'))\n","  component_list = interval_grabber(df)\n","  for interval_dfs in component_list:\n","    for interval_id, interval_df in interval_dfs.items():\n","      interval_df = denoise(interval_df)\n","      interval_df.to_csv(f'/home/d_nguyen11/Documents/reu_stats_2024/datafiles/{ind}/denoised_ACC_{ind}_{interval_id}.csv')\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c9h5Gj0wuVUr"},"outputs":[],"source":["ind_list = []\n","for i in range(8, 17):\n","  if i < 10:\n","    ind_list.append(f'00{i}')\n","  else:\n","    ind_list.append(f'0{i}')\n","\n","for ind in ind_list:\n","  denoising_intervals(ind)"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOZh4xlVTAL5rsszkCp+KcB"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}